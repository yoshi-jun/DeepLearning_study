{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82aee9be-f03f-40aa-ba54-29dfa25ff8ab",
   "metadata": {},
   "source": [
    "# 8 拡散モデルの理論"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6428464-f829-4d7e-ad3b-c7f66ec89c85",
   "metadata": {},
   "source": [
    "前章でVAEについて学習を行なった。学習した結果手書き文字の雰囲気を感じる画像を生成することに成功した。\n",
    "\n",
    "VAEは生成モデルにおいてニューラルネットワークを使用した一例として紹介した。VAEを元に潜在変数を階層化した「階層型VAE」を導出する。そしてそれをさらに発展させた「拡散モデル」へとすすむ。\n",
    "\n",
    "この流れは興味深く、美しいと著者は述べている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5d02a-3609-48fc-b7ea-f976f05b537c",
   "metadata": {},
   "source": [
    "## 8.1 VAEから拡散モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9bd4db-1afb-42fa-9f49-70bbe840464d",
   "metadata": {},
   "source": [
    "本ステップで導出するのは、Denoising Diffusion Probabilistic Models(DDPM)というモデルである。\n",
    "DDPMの各語は\n",
    "* Denoising:ノイズ除去\n",
    "* Diffusion:拡散\n",
    "* Probabilistic:確率的\n",
    "\n",
    "という意味を持つ。\n",
    "\n",
    "DDPMに似たモデルは数多く提案されており、総称して拡散モデルと呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c3e51f-a829-46c5-aa38-f0433585068a",
   "metadata": {},
   "source": [
    "### 8.1.1 VAEの復習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b469900c-cc9f-47e9-8c6b-ee5896615012",
   "metadata": {},
   "source": [
    "VAEは潜在変数を持つモデルである。\n",
    "\n",
    "固定の正規分布から潜在変数をサンプリングし、潜在変数から観測変数への変換をニューラルネットワークで行う。VAEでは、観測変数から潜在変数への変換をニューラルネットワークで行う。\n",
    "\n",
    "具体的には次の式で表した。\n",
    "\n",
    "**デコーダ**：正規分布から潜在変数をサンプリング、潜在変数から観測変数への変換\n",
    "$$p(\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{z};\\boldsymbol{0},\\boldsymbol{I})$$\n",
    "$$\\hat{x} = \\rm{NeuralNet}(\\boldsymbol{z};\\boldsymbol{\\theta})$$\n",
    "$$p(\\boldsymbol{x}|\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{x};\\hat{\\boldsymbol{x}},\\boldsymbol{I})$$\n",
    "\n",
    "**エンコーダ**：観測変数から潜在変数への変換\n",
    "$$\\boldsymbol{\\mu},\\boldsymbol{\\sigma} = \\rm{NeuralNet}(\\boldsymbol{x};\\boldsymbol{\\phi})$$\n",
    "$$q_{\\phi}(\\boldsymbol{z}|\\boldsymbol{x}) = \\mathcal{N}(\\boldsymbol{z};\\boldsymbol{\\mu},\\boldsymbol{\\sigma}^2\\boldsymbol{I})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb4816-99f9-4d48-8e04-d286dcecd1ad",
   "metadata": {},
   "source": [
    "### 8.1.2 潜在変数の階層化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9cb667-5379-4fa0-84ec-f448f12c96c9",
   "metadata": {},
   "source": [
    "上に示すように、潜在変数の数(zはベクトル[1,N])は一つであった。この潜在変数を階層化したモデルが**階層型VAE**である。\n",
    "\n",
    "ここで示す階層型VAEは直前の確率変数から決定される。マルコフ性を仮定するとができるため、パラメータの増加を防ぐことができる。また、潜在変数を階層化することで、複雑な表現をより効率的に表すことができる。\n",
    "\n",
    "仮に、$T$個の潜在変数を持つように階層化した場合を考える。エンコーダ用に$T$個、デコーダ用に$T$個の合計$2T$個のニューラルネットワークが必要になる。\n",
    "\n",
    "そのため、$T$が大きくなると実現が困難になる。実際の階層型VAEの理論と実装について一度眼を通しておく。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f6d970-c24f-4a83-80bf-579a552efda1",
   "metadata": {},
   "source": [
    "### 付録C 階層型VAEの理論と実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59065aa-6169-4c8c-8d2d-572d4efa27b4",
   "metadata": {},
   "source": [
    "二層の潜在変数を持つ階層型VAEについて考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c779de6-10c8-4fb2-bbd4-fe0a318f7daa",
   "metadata": {},
   "source": [
    "#### C.1 ２階層VAEの構成要素"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9693f0cb-2948-4fdb-95b4-de33ce9fc544",
   "metadata": {},
   "source": [
    "階層型のVAEにおいても、単純なVAEと同様にELBOを目的関数とする。VAEのELBOは次の式で表される。\n",
    "\n",
    "$$\\rm{ELBO}(x;\\theta,\\phi) = \\int q_\\phi(z|\\boldsymbol{x}) \\log\\frac{p_\\theta(\\boldsymbol{x},z)}{q_\\phi(z|\\boldsymbol{x})}dz\\\\\n",
    "= \\mathbb{E}_{p_\\theta(\\boldsymbol{x},z)}\\left[\\log\\frac{p_\\theta(\\boldsymbol{x},z)}{q_\\phi(z|\\boldsymbol{x})}\\right]$$\n",
    "\n",
    "二階層VAEのELBOは、$z$に$z_1,z_2$を代入することで得られる。\n",
    "\n",
    "$$\\rm{ELBO}(x;\\theta,\\phi) = \\mathbb{E}_{p_\\theta(\\boldsymbol{x},z_1,z_2)}\\left[\\log\\frac{p_\\theta(\\boldsymbol{x},z_1,z_2)}{q_\\phi(z_1,z_2|\\boldsymbol{x})}\\right]$$\n",
    "\n",
    "上式で出てくる二つの確率分布$p_\\theta(\\boldsymbol{x},z_1,z_2),q_\\phi(z_1,z_2|\\boldsymbol{x})$は確率の乗法定理とマルコフ性より\n",
    "\n",
    "$$p_\\theta(\\boldsymbol{x},z_1,z_2) = p(z_2)p_{\\boldsymbol{\\theta}_2}(z_1|z_2)p_{\\boldsymbol{\\theta}_1}(\\boldsymbol{x}|z_1)$$\n",
    "$$q_\\phi(z_1,z_2|\\boldsymbol{x}) = q_{\\boldsymbol{\\phi}_1}(z_1|\\boldsymbol{x})q_{\\boldsymbol{\\phi}_2}(z_2|z_1)$$\n",
    "\n",
    "上式では$\\boldsymbol{\\theta} = \\{\\boldsymbol{\\theta}_1,\\boldsymbol{\\theta}_2\\}, \\boldsymbol{\\phi}=\\{\\boldsymbol{\\phi}_1,\\{\\boldsymbol{\\phi}_2\\}$で表す。また、確率分布$p(z_2)$は固定の正規分布で次の式で表される。\n",
    "\n",
    "$$p(z_2) = \\mathcal{N}(z_2;\\boldsymbol{0},\\boldsymbol{I})$$\n",
    "\n",
    "デコーダの$p_{\\boldsymbol{\\theta}_2}(z_1|z_2),p_{\\boldsymbol{\\theta}_1}(\\boldsymbol{x}|z_1)$はニューラルネットワークと正規分布を組み合わせて\n",
    "\n",
    "$$\\hat{z} = \\rm{NeuralNet}(z_2;\\boldsymbol{\\theta}_2)\\\\\n",
    "p_{\\boldsymbol{\\theta}_2}(z_1|z_2) = \\mathcal{N}(z_1;\\hat{\\boldsymbol{z}},\\boldsymbol{I})$$\n",
    "\n",
    "$$\\hat{\\boldsymbol{x}} = \\rm{NeuralNet}(z_1;\\boldsymbol{\\theta}_1)\\\\\n",
    "p_{\\boldsymbol{\\theta}_1}(\\boldsymbol{x}|z_1) = \\mathcal{N}(\\boldsymbol{x};\\hat{\\boldsymbol{x}},\\boldsymbol{I})$$\n",
    "\n",
    "と表される。エンコーダの$q_{\\boldsymbol{\\phi}_1}(z_1|\\boldsymbol{x})q_{\\boldsymbol{\\phi}_2}(z_2|z_1)$は\n",
    "\n",
    "$$\\boldsymbol{\\mu}_1,\\boldsymbol{\\sigma}_1 = \\rm{NeuralNet}(\\boldsymbol{x};\\boldsymbol{\\phi}_1)\\\\\n",
    "q_{\\boldsymbol{\\phi}_1}(z_1|\\boldsymbol{x}) = \\mathcal{N}(z_1;\\boldsymbol{\\mu}_1,\\boldsymbol{\\sigma}^2_1\\boldsymbol{I})$$\n",
    "\n",
    "$$\\boldsymbol{\\mu}_2,\\boldsymbol{\\sigma}_2 = \\rm{NeuralNet}(z_1;\\boldsymbol{\\phi}_2)\\\\\n",
    "q_{\\boldsymbol{\\phi}_2}(z_2|z_1) = \\mathcal{N}(z_2;\\boldsymbol{\\mu}_2,\\boldsymbol{\\sigma}^2_2\\boldsymbol{I})$$\n",
    "\n",
    "とモデル化される。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95428da7-ca46-4c03-8799-97bc30f1f9f4",
   "metadata": {},
   "source": [
    "#### C.2 ELBOの式展開"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd9685-8404-409f-ac8a-64f319ba4e9a",
   "metadata": {},
   "source": [
    "以降の式では、パラメータ表記を簡略化する。具体的には、$\\theta_1,\\phi_2$の下つき文字を省略する。\n",
    "\n",
    "$$\\rm{ELBO}(\\boldsymbol{x};\\boldsymbol{\\theta},\\boldsymbol{\\phi}) = \\mathbb{E}_{q_\\phi(z_1,z_2|\\boldsymbol{x})}\\left[\\log\\frac{p_\\theta(\\boldsymbol{x},z_1,z_2)}{q_\\phi(z_1,z_2|\\boldsymbol{x})}\\right]$$\n",
    "先の式より二つの確率分布$p_\\theta(\\boldsymbol{x},z_1,z_2),q_\\phi(z_1,z_2|\\boldsymbol{x})$を書き換え整理すると\n",
    "\n",
    "$$\\rm{ELBO}(\\boldsymbol{x};\\boldsymbol{\\theta},\\boldsymbol{\\phi}) = \\mathbb{E}_{q_\\phi(z_1,z_2|\\boldsymbol{x})}\\left[\\log\\left( p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)\\cdot\\frac{p(z_2)}{q_\\boldsymbol{\\phi}(z_2|z_1)}\\cdot\\frac{p_\\boldsymbol{\\theta}(z_1|z_2)}{q_\\boldsymbol{\\phi}(z_1|\\boldsymbol{x})}\\right)\n",
    "\\right]\\\\ \n",
    "= \\mathbb{E}_{q_\\phi(z_1,z_2|\\boldsymbol{x})}\\left[\\log\\left( p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)\\right)\\right] + \\mathbb{E}_{q_\\phi(z_1,z_2|\\boldsymbol{x})}\\left[\\log\\left(\\frac{p(z_2)}{q_\\boldsymbol{\\phi}(z_2|z_1)}\\right)\\right] + \\mathbb{E}_{q_\\phi(z_1,z_2|\\boldsymbol{x})}\\left[\\log\\left(\\frac{p_\\boldsymbol{\\theta}(z_1|z_2)}{q_\\boldsymbol{\\phi}(z_1|\\boldsymbol{x})}\\right)\\right]$$\n",
    "\n",
    "三つの項で表すことができる。それぞれの項を$J_1,J_2,J_3$としてそれぞれ式展開を行う。$J_1$は\n",
    "\n",
    "$$J_1 = \\mathbb{E}_{q_\\phi(z_1,z_2|\\boldsymbol{x})}\\left[\\log\\left( p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)\\right)\\right]\\\\\n",
    " = \\int q_\\phi(z_1,z_2|\\boldsymbol{x})\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)dz_1dz_2\\\\\n",
    " = \\int q_\\phi(z_1|\\boldsymbol{x})q_\\phi(z_2|\\boldsymbol{x})\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)dz_1dz_2\\\\\n",
    " = \\int q_\\phi(z_1|\\boldsymbol{x})\\int q_\\phi(z_2|\\boldsymbol{x})dz_2 \\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)dz_1\\\\\n",
    " = \\int q_\\phi(z_1|\\boldsymbol{x})\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)dz_1\\\\\n",
    " = \\mathbb{E}_{q_\\phi(z_1|\\boldsymbol{x})}\\left[ \\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)\\right]$$\n",
    "\n",
    "と展開される。途中で$\\int q_\\phi(z_2|\\boldsymbol{x})dz_2 = 1$を使用している点に注意。他の係数は$z_2$に依らないためこの部分のみ$z_2$で積分をすれば良い。\n",
    "次に、$J_2$は次のように展開できる。\n",
    "\n",
    "$$J_2 = \\mathbb{E}_{q_\\phi(z_1,z_2|\\boldsymbol{x})}\\left[\\log\\left(\\frac{p(z_2)}{q_\\boldsymbol{\\phi}(z_2|z_1)}\\right)\\right]\\\\\n",
    "= \\int q_\\phi(z_1,z_2|\\boldsymbol{x})\\log\\frac{p(z_2)}{q_\\boldsymbol{\\phi}(z_2|z_1)}dz_1dz_2\\\\\n",
    "= \\int q_\\phi(z_2|z_1)q_\\phi(z_1|\\boldsymbol{x})\\log\\frac{p(z_2)}{q_\\boldsymbol{\\phi}(z_2|z_1)}dz_1dz_2\\\\\n",
    "= -\\int q_\\phi(z_2|z_1)q_\\phi(z_1|\\boldsymbol{x})\\log\\frac{q_\\boldsymbol{\\phi}(z_2|z_1)}{p(z_2)}dz_1dz_2\\\\\n",
    "= -\\mathbb{E}_{q_\\phi(z_1|\\boldsymbol{x})}\\left[ D_\\rm{KL}(q_\\phi(z_1|\\boldsymbol{x})||p(z_2))\\right]$$\n",
    "\n",
    "途中でKLダイバージェンスが登場している。最後に$J_3$の項は\n",
    "\n",
    "$$J_3 = \\mathbb{E}_{q_\\phi(z_1,z_2|\\boldsymbol{x})}\\left[\\log\\left(\\frac{p_\\boldsymbol{\\theta}(z_1|z_2)}{q_\\boldsymbol{\\phi}(z_1|\\boldsymbol{x})}\\right)\\right]\\\\\n",
    " = \\int q_\\phi(z_1,z_2|\\boldsymbol{x})\\log\\frac{p_\\boldsymbol{\\theta}(z_1|z_2)}{q_\\boldsymbol{\\phi}(z_1|\\boldsymbol{x})}dz_1dz_2\\\\\n",
    " = \\int q_\\phi(z_1|\\boldsymbol{x})q_\\phi(z_2|z_1)\\log\\frac{p_\\boldsymbol{\\theta}(z_1|z_2)}{q_\\boldsymbol{\\phi}(z_1|\\boldsymbol{x})}dz_1dz_2\\\\\n",
    " = - \\int q_\\phi(z_1|\\boldsymbol{x})q_\\phi(z_2|z_1)\\log\\frac{q_\\boldsymbol{\\phi}(z_1|\\boldsymbol{x})}{p_\\boldsymbol{\\theta}(z_1|z_2)}dz_1dz_2\n",
    " = -\\mathbb{E}_{q_\\phi(z_2|\\boldsymbol{x})}\\left[D_{\\rm{KL}}(q_\\boldsymbol{\\phi}(z_1|\\boldsymbol{x})||p_\\boldsymbol{\\theta}(z_1|z_2)\\right]$$\n",
    " \n",
    "と展開できる。$J_2$も同様にKLダイバージェンスが登場する。\n",
    "\n",
    "以上３つの項をまとめると\n",
    "\n",
    "$$\\rm{ELBO}(\\boldsymbol{x};\\boldsymbol{\\theta},\\boldsymbol{\\phi}) = \\mathbb{E}_{q_\\phi(z_1|\\boldsymbol{x})}\\left[ \\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)\\right] -\\mathbb{E}_{q_\\phi(z_1|\\boldsymbol{x})}\\left[ D_\\rm{KL}(q_\\phi(z_1|\\boldsymbol{x})||p(z_2))\\right]-\\mathbb{E}_{q_\\phi(z_2|z_1)}\\left[D_{\\rm{KL}}(q_\\boldsymbol{\\phi}(z_1|\\boldsymbol{x})||p_\\boldsymbol{\\theta}(z_1|z_2)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76d113-2e62-4999-94f7-8c601180880e",
   "metadata": {},
   "source": [
    "#### C.3 モンテカルロ法によるELBOの近似"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb62ba-1f6c-4f74-bd00-8f3391862ef9",
   "metadata": {},
   "source": [
    "ELBOは三つの期待値の項からなる。期待値の計算はモンテカルロ法によって近似できる。今回は、$z_1,z_2$をサンプリングする。$z_1,z_2\\sim q_\\phi(z_1,z_2|\\boldsymbol{x})$計算グラフが途切れないように、変数変換トリックを使用する。期待値の計算は数式を用いて\n",
    "\n",
    "$$\\rm{ELBO}(\\boldsymbol{x};\\boldsymbol{\\theta},\\boldsymbol{\\phi})\\approx \\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)-D_\\rm{KL}(q_\\phi(z_1|\\boldsymbol{x})||p(z_2))-D_{\\rm{KL}}(q_\\boldsymbol{\\phi}(z_1|\\boldsymbol{x})||p_\\boldsymbol{\\theta}(z_1|z_2)$$\n",
    "\n",
    "と表すことができる。この計算式は、サンプルサイズ（モンテカルロ法のサンプリング回数）が１の場合を意味する。経験的には一つのサンプルサイズでも上手くいくことが多い。\n",
    "\n",
    "\n",
    "続いて、各項の計算方法について考えていく。\n",
    "\n",
    "1. 再構成誤差：$\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)$\n",
    "\n",
    "潜在変数$z_1$は、元データ$\\boldsymbol{x}$から$q(z_1|\\boldsymbol{x})$によってサンプリングされる。サンプリングされた$z_1$から$p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)$によって再度データを生成し、それが元データとどれだけ似ているかを評価する。VAEでも登場する再構成誤差項である。なお、$p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)$は次の式で表される。\n",
    "\n",
    "$$\\hat{x} = \\rm{NeuralNet}(z_1;\\boldsymbol{\\theta})\\\\\n",
    "p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)=\\mathcal{N}(\\boldsymbol{x};\\hat{\\boldsymbol{x}},\\boldsymbol{I})$$\n",
    "\n",
    "そして、$\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)=\\log \\mathcal{N}(\\boldsymbol{x};\\hat{\\boldsymbol{x}},\\boldsymbol{I})$と、正規分布に$\\log$が付く。結果として、二乗誤差と定数項の和として計算できる。\n",
    "\n",
    "$$\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}|z_1)=-\\frac{1}{2}\\sum^D_{d=1}(x_d-\\hat{x}_d)^2+\\rm{const}$$\n",
    "\n",
    "ここで、$D$は$\\boldsymbol{x}$の次元数を意味する。\n",
    "\n",
    "2. 事前分布への整合性項:$D_\\rm{KL}(q_\\phi(z_1|\\boldsymbol{x})||p(z_2))$\n",
    "\n",
    "$p(z_2)$は固定の分布（$\\mathcal{N}(z_2;\\boldsymbol{0},\\boldsymbol{I})$）なので、この項は、$q_\\boldsymbol{\\phi}(z_2|z_1)$を事前分布に近づける。効果を持つ。\n",
    "\n",
    "$$q_\\boldsymbol{\\phi}(z_2|z_1) = \\mathcal{N}(z_2;\\boldsymbol{\\mu}_2,\\boldsymbol{\\sigma}_2^2\\boldsymbol{I})$$\n",
    "$$p(z_2) = \\mathcal{N}(z_2;\\boldsymbol{0},\\boldsymbol{I})$$\n",
    "\n",
    "KLダイバージェンスは解析的に求めることができる。次の式で表せる。\n",
    "\n",
    "$$D_\\rm{KL}(q_\\phi(z_2|z_1)||p(z_2)) = -\\frac{1}{2}\\sum^H_{h=1}(1+\\log\\sigma^2_{2,h}-\\mu^2_{2,h}-\\sigma^2_{2,h})$$\n",
    "\n",
    "3. 整合性項:$D_\\rm{KL}(q_\\phi(z_2|z_1)||p(z_2))$\n",
    "\n",
    "潜在変数を階層化したことで、新たに出現する項である。\n",
    "\n",
    "$$q_\\boldsymbol{\\phi}(z_1,z_2) = \\mathcal{N}(z_1;\\boldsymbol{\\mu}_1,\\boldsymbol{\\sigma}^2_1\\boldsymbol{I})\\\\\n",
    "p_\\boldsymbol{\\theta}(z_1,z_2) = \\mathcal{N}(z_1;\\hat{z},\\boldsymbol{I})$$\n",
    "\n",
    "この二つの正規分布のKLダイバージェンスは次の式で表される。\n",
    "\n",
    "$$D_\\rm{KL}(q_\\phi(z_1|\\boldsymbol{x})||p_\\boldsymbol{\\theta}(z_1||z_2)) = -\\frac{1}{2}\\sum^H_{h=1}(1+\\log\\sigma^2_{1,h}-(\\mu_{1,h}-\\hat{z}_h)^2-\\sigma^2_{1,h})$$\n",
    "\n",
    "以上をまとめると、ELBOの計算式は次の式で表される。\n",
    "\n",
    "$$\\rm{ELBO}(\\boldsymbol{x};\\boldsymbol{\\theta},\\boldsymbol{\\phi})\\approx-\\frac{1}{2}\\sum^D_{d=1}(x_d-\\hat{x}_d)^2+\\rm{const}+\\frac{1}{2}\\sum^H_{h=1}(1+\\log\\sigma^2_{2,h}-\\mu^2_{2,h}-\\sigma^2_{2,h})+\\frac{1}{2}\\sum^H_{h=1}(1+\\log\\sigma^2_{1,h}-(\\mu_{1,h}-\\hat{z}_h)^2-\\sigma^2_{1,h})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86aa34a-8a5c-4e36-95b0-037755c87d92",
   "metadata": {},
   "source": [
    "#### C.4 ２階層のVAEの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ce503-cf8b-467b-baf5-3f9dfbb0d368",
   "metadata": {},
   "source": [
    "次に二回層VAEの実装を行う。ELBO全体を２倍した値を損失関数とする。\n",
    "\n",
    "$$\\rm{Loss}(\\boldsymbol{x};\\boldsymbol{\\theta},\\boldsymbol{\\phi}) = \\sum^D_{d=1}(x_d-\\hat{x}_d)^2-\\sum^H_{d=1}(1+\\log\\sigma^2_{2,h}-\\mu^2_{2,h}-\\sigma^2_{2,h})-\\sum^H_{h=1}(1+\\log\\sigma^2_{1,h}-(\\mu_{1,h}-\\hat{z}_h)^2-\\sigma^2_{1,h})$$\n",
    "\n",
    "参考までに、計算グラフは図C-4のようになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce2084-5aba-446a-905d-6f80e54d9aeb",
   "metadata": {},
   "source": [
    "#### C.5 実装コード"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee43ac-930f-4b64-99cb-c2843ae0b635",
   "metadata": {},
   "source": [
    "インポートとネットワークの実装を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda2a93-109a-48c6-81eb-f960e193418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# import torchvision\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "input_dim = 784  # mnist image 28x28\n",
    "hidden_dim = 100\n",
    "latent_dim = 20\n",
    "epochs = 30\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# class Encoder(nn.Module):                                                      # エンコーダの実装\n",
    "#     def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "#         super().__init__()\n",
    "#         self.linear = nn.Linear(input_dim, hidden_dim)                         # 一層目\n",
    "#         self.linear_mu = nn.Linear(hidden_dim, latent_dim)                     # 二層目（平均）\n",
    "#         self.linear_logvar = nn.Linear(hidden_dim, latent_dim)                 # 二層目(log sigma)分散\n",
    "\n",
    "#     def forward(self, x):                                                      # 順伝搬\n",
    "#         h = self.linear(x)                                                     # 1度目の線形変換\n",
    "#         h = F.relu(h)                                                          # 非線形変換\n",
    "#         mu = self.linear_mu(h)                                                 # 平均　出力\n",
    "#         logvar = self.linear_logvar(h)                                         # log sigma 出力\n",
    "#         sigma = torch.exp(0.5 * logvar)                                        # sigma を計算\n",
    "#         return mu, sigma\n",
    "\n",
    "\n",
    "# class Decoder(nn.Module):                                                      # デコーダの実装\n",
    "#     def __init__(self, latent_dim, hidden_dim, output_dim, use_sigmoid=False): \n",
    "#         super().__init__()\n",
    "#         self.linear1 = nn.Linear(latent_dim, hidden_dim)                       # 一層目\n",
    "#         self.linear2 = nn.Linear(hidden_dim, output_dim)                       # 二層目\n",
    "#         self.use_sigmoid = use_sigmoid                                         # シグモイド関数\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         h = self.linear1(z)                                                    # 一度目の線形変換\n",
    "#         h = F.relu(h)                                                          # 非線形変換\n",
    "#         h = self.linear2(h)                                                    # 二度目の線形変換\n",
    "#         if self.use_sigmoid:\n",
    "#             h = F.sigmoid(h)\n",
    "#         return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330bdd7-b392-43b4-84e8-bfadfa38b6e9",
   "metadata": {},
   "source": [
    "シグモイド関数を利用するか否かはフラグで実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39720980-533e-4927-a363-ef0a486ed223",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     z \u001b[38;5;241m=\u001b[39m mu \u001b[38;5;241m+\u001b[39m eps \u001b[38;5;241m*\u001b[39m sigma                                                                \u001b[38;5;66;03m# パラメータmu, sigma の正規分布からサンプリング\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mVAE\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):                                                                   \u001b[38;5;66;03m# VAEの実装\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim, hidden_dim, latent_dim):\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "def reparameterize(mu, sigma):                                                          # 変数変換トリックの実装\n",
    "    eps = torch.randn_like(sigma)                                                       # 乱数を作成\n",
    "    z = mu + eps * sigma                                                                # パラメータmu, sigma の正規分布からサンプリング\n",
    "    return z\n",
    "\n",
    "\n",
    "class VAE(nn.Module):                                                                   # VAEの実装\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder1 = Encoder(input_dim, hidden_dim, latent_dim)                      # 一つ目のエンコーダ\n",
    "        self.encoder2 = Encoder(latent_dim, hidden_dim, latent_dim)                     # 二つ目のエンコーダ\n",
    "        self.decoder1 = Decoder(latent_dim, hidden_dim, input_dim, use_sigmoid=True)    # 一つ目のデコーダ　シグモイドを使用\n",
    "        self.decoder2 = Decoder(latent_dim, hidden_dim, latent_dim)                     # 二つ目のデコーダ シグモイドは使用しない\n",
    "\n",
    "    def get_loss(self, x):                                                              # 損失の計算\n",
    "        mu1, sigma1 = self.encoder1(x)                                                  # エンコーダから分布のパラメータを推論\n",
    "        z1 = reparameterize(mu1, sigma1)                                                # サンプリング\n",
    "        mu2, sigma2 = self.encoder2(z1)                                                 # エンコーダ２から確率分布のパラメータを推論\n",
    "        z2 = reparameterize(mu2, sigma2)                                                # サンプリング\n",
    "\n",
    "        z_hat = self.decoder2(z2)                                                       # デコーダから出力\n",
    "        x_hat = self.decoder1(z1)\n",
    "\n",
    "        # loss\n",
    "        batch_size = len(x)\n",
    "        L1 = F.mse_loss(x_hat, x, reduction='sum')                                      # 二乗平均誤差\n",
    "        L2 = - torch.sum(1 + torch.log(sigma2 ** 2) - mu2 ** 2 - sigma2 ** 2)           # L2ノルムを計算\n",
    "        L3 = - torch.sum(1 + torch.log(sigma1 ** 2) - (mu1 - z_hat) ** 2 - sigma1 ** 2) # L3ノルムを計算\n",
    "        return (L1 + L2 + L3) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f418c-04fb-4587-96d1-5071091445de",
   "metadata": {},
   "source": [
    "謎の病で実行できない。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdac2eb-362e-460d-8b5e-82b60a07658d",
   "metadata": {},
   "source": [
    "### 8.1.3 拡散モデルへ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74280fc-1bf6-4785-b33a-c9cdf6690559",
   "metadata": {},
   "source": [
    "実装を行った、階層型VAEを拡散モデルと進化させる。**変更点は2つだけ！！**\n",
    "\n",
    "* 観測変数と潜在変数の次元数を同じにする。\n",
    "* エンコーダは、固定の正規分布によるノイズを追加する。\n",
    "\n",
    "ノイズの除去と、ノイズの追加を繰り返す。追加するノイズに潜在変数を使用する。$x_0$が観測変数であり、残りの$x_1,x_2,\\cdots, x_T$は潜在変数を意味する。\n",
    "拡散モデルの$q$は固定のガウスノイズを追加するだけであり、パラメータは不要である。\n",
    "\n",
    "拡散モデルは、ノイズを除去する処理をニューラルネットワークでモデル化する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32419274-a0e3-4b0d-a1d7-bdd4d66fdf42",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8.2 拡散過程と逆拡散過程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702fcf62-2ee6-46a1-865f-17162721497c",
   "metadata": {},
   "source": [
    "拡散モデルは、二つの処理がある。一つはノイズを追加する処理である。粒子がランダムな運動によって均一に広がる現象になぞらえて、**拡散過程**と呼ばれる。もう一つはノイズを消去する逆拡散過程と呼ばれる処理である。この二つの過程について見ていく。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc98ebfe-819c-4815-a816-0f6681ef4cd7",
   "metadata": {},
   "source": [
    "### 8.2.1 拡散過程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6efc8-e5fa-423d-b71e-486110686888",
   "metadata": {},
   "source": [
    "先に述べたように、拡散過程ではノイズを加える。加えるノイズは、最終時刻$T$における変数$\\boldsymbol{x}$が完全なノイズになる必要がある。\n",
    "\n",
    "最終的に、$\\mathcal{N}(\\boldsymbol{x}_T;\\boldsymbol{0},\\boldsymbol{I})$に従うように、各時刻$t$で加えるノイズを調整する。\n",
    "\n",
    "$$q(\\boldsymbol{x}_t|\\boldsymbol{x}_{t-1}=\\mathcal{N}(\\boldsymbol{x}_t;\\sqrt{1-\\beta_t}\\boldsymbol{x}_{t-1},\\beta_t\\boldsymbol{I})$$\n",
    "\n",
    "ここで、$t$は$1<t<T$の整数とする。また$\\beta_t$は任意のパラメータであり、加えるガウスノイズの大きさを決める。\n",
    "\n",
    "ガウスノイズの和はガウスノイズになるため（詳細な計算は8.4でのべる）、最終的に$p(\\boldsymbol{x}_T) \\approx \\mathcal{N}(\\boldsymbol{x}_T;\\boldsymbol{0},\\boldsymbol{I})$となる。\n",
    "\n",
    "上式の正規分布のサンプリングは、変数変換トリックを使用すると\n",
    "\n",
    "$$\\epsilon\\sim\\mathcal{N}(\\epsilon;\\boldsymbol{0},\\boldsymbol{I})\\\\\n",
    "\\boldsymbol{x}_t = \\sqrt{1-\\beta_t}\\boldsymbol{x}_{t-1}+\\sqrt{\\beta_t}\\epsilon$$\n",
    "\n",
    "と書き換えることができる。$\\beta = 0.01$の値を設定した場合、$\\boldsymbol{x}_t = 0.995\\boldsymbol{x}_{t-1}+0.1\\epsilon$となる。前データ$\\boldsymbol{x}_t$をスケールダウンし、そこに小さなノイズ$\\epsilon\\sim\\mathcal{N}(\\epsilon;\\boldsymbol{0},\\boldsymbol{I})$を加えることがわかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ba28d-af03-4264-b17a-de129824925b",
   "metadata": {},
   "source": [
    "### 8.2.2 逆拡散過程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6186d02-faf5-48f2-85c7-f0c22247cd10",
   "metadata": {},
   "source": [
    "逆拡散過程は、先ほど加えたノイズを除去する処理である。この処理は、ニューラルネットワークで行う。\n",
    "\n",
    "各時刻$t$におけるノイズを除去する処理を、個別のニューラルネットワークで行う場合、$T$個のニューラルネットワークを用意する必要がある。しかし、各時刻$t$における出力の次元数$\\dim{\\boldsymbol{x}_t}$は全て同じである。そのため、ニューラルネットワークの構造を共通化することができる。また、時刻$t$を入力信号として与えることで、各時刻を個別に処理させることができる(one-hotラベルをつける？)。\n",
    "\n",
    "ニューラルネットワークにデータ$\\boldsymbol{x}_t$と時刻$t$を入力しノイズを除去した$\\hat{\\boldsymbol{x}}_{t-1}$を入力とする。この$\\hat{\\boldsymbol{x}}_{t-1}$は$\\boldsymbol{x}_{t-1}$の推定値を意味する。これを数式で\n",
    "\n",
    "$$\\hat{\\boldsymbol{x}}_{t-1} = \\rm{NeuralNet}(\\boldsymbol{x}_t,t;\\boldsymbol{\\theta})\\\\\n",
    "p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t) = \\mathcal{N}(\\boldsymbol{x}_{t-1};\\hat{\\boldsymbol{x}}_{t-1},\\boldsymbol{I})$$\n",
    "\n",
    "と表される。ここでは、簡単のため共分散行列を単位行列$I$とする。\n",
    "\n",
    "**まとめ**\n",
    "\n",
    "拡散モデルでは、ノイズの追加と除去を繰り返す。ノイズを追加することを拡散過程、除去することを逆拡散過程と呼ぶ。\n",
    "\n",
    "加えるノイズは最終時刻$T$において完全なノイズになる必要があり、追加するノイズは次の式で表される。\n",
    "\n",
    "$$\\epsilon\\sim\\mathcal{N}(\\epsilon;\\boldsymbol{0},\\boldsymbol{I})\\\\\n",
    "\\boldsymbol{x}_t = \\sqrt{1-\\beta_t}\\boldsymbol{x}_{t-1}+\\sqrt{\\beta_t}\\epsilon$$\n",
    "\n",
    "次に、ニューラルネットワークを用いて逆拡散を行う。ニューラルネットワークの入力は、時刻$t$とデータ$x_{t-1}$であり、出力は推論した$\\hat{\\boldsymbol{x}}_{t-1}$である。これを平均ベクトルとして正規分布からサンプリングする。\n",
    "\n",
    "$$\\hat{\\boldsymbol{x}}_{t-1} = \\rm{NeuralNet}(\\boldsymbol{x}_t,t;\\boldsymbol{\\theta})\\\\\n",
    "p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t) = \\mathcal{N}(\\boldsymbol{x}_{t-1};\\hat{\\boldsymbol{x}}_{t-1},\\boldsymbol{I})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9dba6-881e-4971-a6a7-bd6efbee1d1e",
   "metadata": {},
   "source": [
    "## 8.3 ELBOの計算①"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096516a8-d3ce-4c6f-829a-f674934ed7db",
   "metadata": {},
   "source": [
    "対数尤度が最大となるパラメータを探索することが難しいため、対数尤度の下限であるELBOを最適化の対象とする。\n",
    "\n",
    "３段階のゴールを設けて進む。\n",
    "\n",
    "1. サンプルサイズT\n",
    "2. サンプルサイズ2\n",
    "3. サンプルサイズ1\n",
    "\n",
    "徐々にサンプルサイズが小さくなっているが、より工夫が必要とのこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b9b56-a042-43f7-87b1-9267a781e975",
   "metadata": {},
   "source": [
    "### 8.3.1 拡散モデルのELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8048f277-1db4-47c2-b9c0-184df97c8144",
   "metadata": {},
   "source": [
    "拡散モデルのELBOはVAEのELBOと同様に導出ができる。VAEのELBOは次の式で表された。\n",
    "\n",
    "\n",
    "$$\\rm{ELBO}(\\boldsymbol{x};\\boldsymbol{\\theta},\\boldsymbol{\\phi}) = \\int q_\\boldsymbol{\\phi}(\\boldsymbol{z}|\\boldsymbol{x}) \\log\\frac{p_\\boldsymbol{\\theta}(\\boldsymbol{x},\\boldsymbol{z})}{q_\\boldsymbol{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}dz\\\\\n",
    "= \\mathbb{E}_{q_\\boldsymbol{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}\\left[\\log\\frac{p_\\boldsymbol{\\theta}(\\boldsymbol{x},\\boldsymbol{z})}{q_\\boldsymbol{\\phi}(\\boldsymbol{z}|\\boldsymbol{x})}\\right]\n",
    "$$\n",
    "\n",
    "ここから以下の三つの点を変更することで、拡散モデルのELBOを導出することができる。\n",
    "\n",
    "* $\\boldsymbol{x}$を$\\boldsymbol{x}_0$へ変更\n",
    "* $z$を$\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\cdots,\\boldsymbol{x}_T$へ変更\n",
    "* パラメータ$\\phi$を消去\n",
    "\n",
    "この3点を変更すると次の式になる。\n",
    "\n",
    "$$\\rm{ELBO}(\\boldsymbol{x}_0;\\boldsymbol{\\theta})=\\mathbb{E}_{(\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\cdots,\\boldsymbol{x}_T|\\boldsymbol{x}_0)}\\left[\\log\\frac{p_\\boldsymbol{\\theta}(\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\cdots,\\boldsymbol{x}_T)}{q_\\boldsymbol{\\phi}(\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\cdots,\\boldsymbol{x}_T|\\boldsymbol{x}_0)}\\right]$$\n",
    "\n",
    "数式の表記を簡略化するため、表記を$\\boldsymbol{x}_{0:T}=\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\cdots,\\boldsymbol{x}_T$とする。この表記を用いると上式は\n",
    "\n",
    "$$\\rm{ELBO}(\\boldsymbol{x}_0;\\boldsymbol{\\theta})=\\mathbb{E}_{(\\boldsymbol{x}_{0:T}|\\boldsymbol{x}_0)}\\left[\\log\\frac{p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0:T})}{q_\\boldsymbol{\\phi}(\\boldsymbol{x}_{0:T}|\\boldsymbol{x}_0)}\\right]$$\n",
    "\n",
    "と書き換えることができる。\n",
    "\n",
    "**イェセンの不等式を用いたELBOの導出**\n",
    "\n",
    "拡散モデルのELBOはイェセンの不等式を用いても導出することができる。イェセンの不等式は次の式で表される。\n",
    "\n",
    "$$\\mathbb{E}_{q(x)}[\\log f(x)]\\leq\\log\\mathbb{E}_{q(x)}[f(x)]$$\n",
    "\n",
    "これを用いて、拡散モデルのELBOは次のように導出できる。\n",
    "\n",
    "$$\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0)\n",
    " = \\log\\int p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0:T})d\\boldsymbol{x}_{1:T}\\\\\n",
    " = \\log\\int p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0:T})\\frac{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}d\\boldsymbol{x}_{1:T}\\\\\n",
    " = \\log \\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\frac{ p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0:T})}{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\right]\\\\\n",
    " \\geq \\mathbb{E}_{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\log \\frac{ p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0:T})}{q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\right]\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6ea80-79b2-421d-ac58-0d41e9277abc",
   "metadata": {},
   "source": [
    "### 8.3.2 ELBOの式展開"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656aee7-101f-43e4-9a91-7540883af149",
   "metadata": {},
   "source": [
    "計算したELBOの式展開を行う。\n",
    "\n",
    "乗法定理とマルコフ性により次の式で表される。\n",
    "\n",
    "$$ p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0:T})=p_\\boldsymbol{\\theta}(\\boldsymbol{x}_0|\\boldsymbol{x}_1)(\\boldsymbol{x}_1|\\boldsymbol{x}_2)\\cdots(\\boldsymbol{x}_{T-1}|\\boldsymbol{x}_T)p(\\boldsymbol{x}_T)\\\\\n",
    "= p(\\boldsymbol{x}_T)\\prod^T_{t=1}p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)$$\n",
    "\n",
    "ここで、最終時刻の$p(\\boldsymbol{x}_T)$は完全なガウスノイズである$\\mathcal{N}(\\boldsymbol{x}_T;\\boldsymbol{0},\\boldsymbol{I})$を表す。$q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)$も乗法定理とマルコフ性より次の式で表される。\n",
    "\n",
    "$$q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)=\\prod^T_{t=1}q(\\boldsymbol{x}_t|\\boldsymbol{x}_{t-1})$$\n",
    "\n",
    "この二つの式よりELBOは次の式で表される。\n",
    "\n",
    "$$\\rm{ELBO}(\\boldsymbol{x}_0;\\boldsymbol{\\theta})=\\mathbb{E}_{(\\boldsymbol{x}_{0:T}|\\boldsymbol{x}_0)}\\left[\\log\\frac{p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{0:T})}{q_\\boldsymbol{\\phi}(\\boldsymbol{x}_{0:T}|\\boldsymbol{x}_0)}\\right]\\\\\n",
    "=\\mathbb{E}_{(\\boldsymbol{x}_{0:T}|\\boldsymbol{x}_0)}\\left[\\log\\frac{p(\\boldsymbol{x}_T)\\prod^T_{t=1}p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)}{\\prod^T_{t=1}q(\\boldsymbol{x}_t|\\boldsymbol{x}_{t-1})}\\right]\\\\ =\\mathbb{E}_{(\\boldsymbol{x}_{0:T}|\\boldsymbol{x}_0)}\\left[\\log{\\prod^T_{t=1}p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)}+\\log\\frac{p(\\boldsymbol{x}_T)}{\\prod^T_{t=1}q(\\boldsymbol{x}_t|\\boldsymbol{x}_{t-1})}\\right]$$\n",
    "\n",
    "ここで、$\\boldsymbol{\\theta}$を含まない項を無視できる。目的関数$J(\\boldsymbol{\\theta})$は\n",
    "\n",
    "$$J(\\boldsymbol{\\theta}) = \\mathbb{E}_{(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[\\log{\\prod^T_{t=1}p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)}\\right]\\\\\n",
    "= \\mathbb{E}_{(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_0)}\\left[{\\sum^T_{t=1}\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)}\\right]$$\n",
    "\n",
    "と表される。ここで、期待値はモンテカルロ法によって近似できる。今回の場合、$\\boldsymbol{x}_{1:T}$をいくつか生成し、その時の期待値の中身$\\sum^T_{t=1}\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)$の平均を求める。仮のモンテカルロ法のサンプルサイズを1とすると\n",
    "\n",
    "$$\\boldsymbol{x}_{1:T}\\sim q(\\boldsymbol{x}_{1:T}|\\boldsymbol{x}_T)\\\\\n",
    "J(\\boldsymbol{\\theta})\\approx\\sum^T_{t=1}\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)$$\n",
    "\n",
    "まずは拡散課程により、元のデータ$x_0$から始めて$\\boldsymbol{x}_{1:T}$を生成する。これを利用して各時刻の対数尤度を計算する。\n",
    "\n",
    "$$\\hat{\\boldsymbol{x}}=\\rm{NeuralNet}(\\boldsymbol{x}_t,t;\\boldsymbol{\\theta})\\\\\n",
    "p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)=\\mathcal{N}(\\boldsymbol{x}_{t-1};\\hat{\\boldsymbol{x}}_{t-1},\\boldsymbol{I})$$\n",
    "\n",
    "これにより目的関数$J(\\boldsymbol{\\theta})$は次の式で表される。\n",
    "\n",
    "$$J(\\boldsymbol{\\theta})\\approx\\sum^T_{t=1}\\log p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1}|\\boldsymbol{x}_t)\\\\\n",
    "= \\sum^T_{t=1}\\log\\mathcal{N}(\\boldsymbol{x}_{t-1};\\hat{\\boldsymbol{x}}_{t-1},\\boldsymbol{I})\\\\\n",
    "= \\sum^{T-1}_{t=0}\\log\\mathcal{N}(\\boldsymbol{x}_t;\\hat{\\boldsymbol{x}}_t,\\boldsymbol{I})\\\\\n",
    "= \\sum^{T-1}_{t=0}\\log\\frac{1}{\\sqrt{(2\\pi)^D|\\boldsymbol{I}|}}\\exp{\\left\\{-\\frac{1}{2}(\\boldsymbol{x}_t-\\hat{\\boldsymbol{x}}_t)^\\top\\boldsymbol{I}^{-1}(\\boldsymbol{x}_t-\\hat{\\boldsymbol{x}}_t)\\right\\}}\\\\\n",
    "= \\sum^{T-1}_{t=0}\\left(-\\frac{1}{2}(\\boldsymbol{x}_t-\\hat{\\boldsymbol{x}}_t)^\\top(\\boldsymbol{x}_t-\\hat{\\boldsymbol{x}}_t)+\\log\\frac{1}{\\sqrt{(2\\pi)^D}}\\right)\\\\\n",
    "=-\\frac{1}{2}\\sum^{T-1}_{t=0}(\\boldsymbol{x}_t-\\hat{\\boldsymbol{x}}_t)^\\top(\\boldsymbol{x}_t-\\hat{\\boldsymbol{x}}_t)+T\\log\\frac{1}{\\sqrt{(2\\pi)^D}}\n",
    "$$\n",
    "\n",
    "最後の項は定数項であるため、$J(\\boldsymbol{\\theta}$は\n",
    "\n",
    "$$J(\\boldsymbol{\\theta})\\approx-\\frac{1}{2}\\sum^{T-1}_{t=0}(\\boldsymbol{x}_t-\\hat{\\boldsymbol{x}}_t)^\\top(\\boldsymbol{x}_t-\\hat{\\boldsymbol{x}}_t)\\\\\n",
    "= -\\frac{1}{2}\\sum^{T-1}_{t=0}||\\boldsymbol{x}_t-\\hat{\\boldsymbol{x}}_t||^2$$\n",
    "\n",
    "$||\\cdot||^2$はL2ノルムの二乗であり、各要素を二乗した和を表す。例えば、$\\boldsymbol{x}$が次元数を$D$とすると$||\\boldsymbol{x}||^2$は。\n",
    "\n",
    "$$||\\boldsymbol{x}||^2 = x^2_1+x^2_2+\\cdots+x^2_D$$\n",
    "\n",
    "で表される。\n",
    "\n",
    "**L1ノルムとL2ノルム**\n",
    "\n",
    "L1,L2ノルムはベクトルの長さや大きさを測るための方法である。L1ノルムはマンハッタン距離と呼ばれ次の式で表される。\n",
    "\n",
    "$$||\\boldsymbol{x}||_1=|x_1|+|x_2|+\\cdots+|x_D|$$\n",
    "\n",
    "L2ノルムはユークリッド距離と呼ばれ次の式で表される。\n",
    "\n",
    "$$||\\boldsymbol{x}||_2 = \\sqrt{x^2_1+x^2_2+\\cdots+x^2_D}$$\n",
    "\n",
    "よって、L2ノルムの二乗は次の式で表される。\n",
    "\n",
    "$$||\\boldsymbol{x}||_2^2 = x^2_1+x^2_2+\\cdots+x^2_D$$\n",
    "\n",
    "本書では、$||x||$をL2ノルムと表すこととする。\n",
    "\n",
    "以上で、目的関数$J(\\boldsymbol{\\theta})を求めることができました。その方法を求めると\n",
    "\n",
    "1. 拡散課程により$T$個のサンプリングを行い\n",
    "2. ニューラルネットワークを$T$回適用してノイズ除去を行う\n",
    "3. 各時刻の二乗誤差$||\\boldsymbol{x}_t-\\hat{\\boldsymbol{x}}_t||^2$を求める\n",
    "\n",
    "この方法では、１回の計算に$T$回のサンプリングが必要になる。このサンプリングの数を少なくする方法が必要になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8debf537-1d63-49d4-aee7-af96b4961e13",
   "metadata": {},
   "source": [
    "## 8.4 ELBOの計算②"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee05bf-aa2a-493b-bb39-4a05dcaa47d6",
   "metadata": {},
   "source": [
    "前節では$T$個のサンプリングデータによって近似を行った。\n",
    "\n",
    "次の目標は2個のサンプリングデータからELBOを近似する方法である。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb70736-254b-4562-a4cb-14306161ca2e",
   "metadata": {},
   "source": [
    "### 8.4.3 $q(x_t|x_0)$の導出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36558e38-d55d-46e6-939d-e9124410db95",
   "metadata": {},
   "source": [
    "まずは、ガウスノイズの和がガウスノイズになるという性質について説明する。\n",
    "\n",
    "今ここに独立に生成された２つのガウスノイズ$x,y$があるとする。その和を$z$とすると\n",
    "\n",
    "$$ x\\sim\\mathcal{N}(\\mu_x,\\sigma_x^2)\\\\\n",
    "y\\sim\\mathcal{N}(\\mu_y,\\sigma_y^2)\\\\\n",
    "z = y+x$$\n",
    "\n",
    "この時、一つの正規分布からのサンプリングとして\n",
    "\n",
    "$$z\\sim\\mathcal{N}(\\mu_x+\\mu_y,\\sigma_x^2,\\sigma_y^2)$$\n",
    "\n",
    "と表すことができる。$z$は平均が$\\mu_x+\\mu_y$、分散が$\\sigma_x^2+\\sigma_y^2$の正規分布に従う。\n",
    "\n",
    "続いて、このガウスノイズの性質を拡散課程に応用する。拡散課程の$q(\\boldsymbol{x}_t|\\boldsymbol{x}_{t-1}$は次の式で表される。\n",
    "\n",
    "$$q(\\boldsymbol{x}_t|\\boldsymbol{x}_{t-1})=\\mathcal{N}(\\boldsymbol{x}_t;\\sqrt{1-\\beta_t}\\boldsymbol{x}_{t-1},\\beta_t\\boldsymbol{I})$$\n",
    "\n",
    "ここでは$\\alpha_t=1-\\beta_t$を用いて\n",
    "\n",
    "$$q(\\boldsymbol{x}_t|\\boldsymbol{x}_{t-1})=\\mathcal{N}(\\boldsymbol{x}_t;\\sqrt{\\alpha_t}\\boldsymbol{x}_{t-1},(1-\\alpha_t)\\boldsymbol{I})$$\n",
    "\n",
    "変数変換トリックを使用すると次の式で表される。\n",
    "\n",
    "$$\\epsilon_t\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})\\\\\n",
    "\\boldsymbol{x}_t=\\sqrt{\\alpha_t}\\boldsymbol{x}_{t-1}+\\sqrt{1-\\alpha_t}\\epsilon_t$$\n",
    "\n",
    "次に、$t$に$t-1$を代入して次の式を得る。\n",
    "\n",
    "$$\\epsilon_{t-1}\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})\\\\\n",
    "\\boldsymbol{x}_{t-1} = \\sqrt{\\alpha_{t-1}}\\boldsymbol{x}_{t-2}+\\sqrt{1-\\alpha_t}\\epsilon_{t-1}$$\n",
    "\n",
    "二つの式から\n",
    "\n",
    "$$\\boldsymbol{x}_t=\\sqrt{\\alpha_t}\\boldsymbol{x}_{t-1}+\\sqrt{1-\\alpha_t}\\epsilon_t\\\\\n",
    "= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1}}\\boldsymbol{x}_{t-2}+\\sqrt{1-\\alpha_t}\\epsilon_{t-1})+\\sqrt{1-\\alpha_t}\\epsilon_t\\\\\n",
    "= \\sqrt{\\alpha_t\\alpha_{t-1}}\\boldsymbol{x}_{t-2}+\\sqrt{\\alpha_t-\\alpha_t\\alpha_{t-1}}\\epsilon_{t-1}+\\sqrt{1-\\alpha_t}\\epsilon_t$$\n",
    "\n",
    "が得られる。ここで、ガウスノイズの和が登場する。$\\epsilon_t,\\epsilon_{t-1}$は$\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})$より独立に生成されたサンプルである。そのため、$\\sqrt{\\alpha_t-\\alpha_t\\alpha_{t-1}}\\epsilon_{t-1}+\\sqrt{1-\\alpha_t}$はガウスノイズの和であり、一つの正規分布で表せる。具体的には\n",
    "\n",
    "$$\\mathcal{N}(\\boldsymbol{0},(1-\\alpha_t\\alpha_{t-1})\\boldsymbol{I})$$\n",
    "\n",
    "からのサンプルであり、変数変換トリックより\n",
    "\n",
    "$$\\epsilon\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})\\\\\n",
    "\\boldsymbol{x}_t=\\sqrt{\\alpha_t\\alpha_{t-1}}\\boldsymbol{x}_{t-2}+\\sqrt{\\alpha_t-\\alpha_t\\alpha_{t-1}}\\epsilon$$\n",
    "\n",
    "あとは、同じ式変形を繰り返し行う。最終的には\n",
    "\n",
    "$$\\boldsymbol{x}_t=\\sqrt{\\alpha_t\\alpha_{t-1}\\cdots\\alpha_1}\\boldsymbol{x}+\\sqrt{1-\\alpha_t\\alpha_{t-1}\\cdots\\alpha_1}\\epsilon\\\\\n",
    "=\\sqrt{\\bar{\\alpha}_t}\\boldsymbol{x}_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon$$\n",
    "\n",
    "ここでは、$\\alpha_t$から$\\alpha_1$までの積を$\\bar{\\alpha}_t$で表す。\n",
    "\n",
    "上式を確率分布の式で\n",
    "\n",
    "$$q(\\boldsymbol{x}_t|\\boldsymbol{x}_0)=\\mathcal{N}(\\boldsymbol{x}_t;\\sqrt{\\bar{\\alpha}_t}\\boldsymbol{x}_0,(1-\\bar{\\alpha}_t)\\boldsymbol{I})$$\n",
    "\n",
    "と表すことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25300ca6-8942-4096-baf5-8d92c688b15b",
   "metadata": {},
   "source": [
    "### 8.4.1 $q(\\boldsymbol{x}_t,\\boldsymbol{x}_0)$の式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb45b4-ddf4-4266-b02a-d4093cc158a2",
   "metadata": {},
   "source": [
    "先に導出した式によって、$q(\\boldsymbol{x}_t,\\boldsymbol{x}_0)$は次のように表せる。\n",
    "\n",
    "$$q(\\boldsymbol{x}_t|\\boldsymbol{x}_0)=\\mathcal{N}(\\boldsymbol{x}_t;\\sqrt{\\bar{\\alpha}_t}\\boldsymbol{x}_0,(1-\\bar{\\alpha}_t)\\boldsymbol{I})$$\n",
    "\n",
    "上式における記号は\n",
    "\n",
    "$$\\alpha_t=1-\\beta_t\\\\\n",
    "\\bar{\\alpha}_t=\\alpha_t\\alpha_{t-1}\\cdots\\alpha_1$$\n",
    "\n",
    "であり、$\\alpha_t=1-\\beta_t$でありユーザーが自由に設定できる値である。解析的に表せることで、ELBOの計算を効率化できることを示す。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053d69e-6591-420c-9653-9096dd045064",
   "metadata": {},
   "source": [
    "### 8.4.2 ELBOの近似解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83fcf0-404e-43f7-bf31-e119ddfec286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
