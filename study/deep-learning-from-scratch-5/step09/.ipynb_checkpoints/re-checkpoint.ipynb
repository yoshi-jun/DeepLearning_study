{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97138890-e11f-402d-b056-082d139fbe28",
   "metadata": {},
   "source": [
    "# モデルの改良"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c476511-d020-4ec2-9cf1-2bfd856014a4",
   "metadata": {},
   "source": [
    "前回の結果をみると、手書き文字の精度として低い。\n",
    "\n",
    "参考までに前回の結果を提示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45824a7c-94de-46a8-b1da-93fde5472ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19%|███████▊                                  | 87/469 [02:34<11:32,  1.81s/it]^C\n",
      " 19%|███████▊                                  | 87/469 [02:35<11:21,  1.78s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yoshidajunichi/Documents/DeepLearning_study/study/deep-learning-from-scratch-5/step09/diffusion_model.py\", line 202, in <module>\n",
      "    noise_pred = model(x_noisy, t)\n",
      "  File \"/Users/yoshidajunichi/.pyenv/versions/anaconda3-2021.11/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/yoshidajunichi/.pyenv/versions/anaconda3-2021.11/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/yoshidajunichi/Documents/DeepLearning_study/study/deep-learning-from-scratch-5/step09/diffusion_model.py\", line 104, in forward\n",
      "    x = torch.cat([x, x1], dim=1)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 diffusion_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0911e66-2ca3-4a41-b335-a4723f674bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_embed_dim):\n",
    "        super().__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(time_embed_dim, in_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_ch, in_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, v):\n",
    "        N, C, _, _ = x.shape\n",
    "        v = self.mlp(v)\n",
    "        v = v.view(N, C, 1, 1)\n",
    "        y = self.convs(x + v)\n",
    "        return y\n",
    "\n",
    "in_ch = 1\n",
    "time_embed_dim=100\n",
    "timesteps = torch.tensor([1])\n",
    "\n",
    "down1 = ConvBlock(in_ch, 64, time_embed_dim)\n",
    "down2 = ConvBlock(64,   128, time_embed_dim)\n",
    "down3 = ConvBlock(128,  256, time_embed_dim)\n",
    "\n",
    "bot1 = ConvBlock(256, 512, time_embed_dim)\n",
    "\n",
    "up3 = ConvBlock(128 + 512, 256, time_embed_dim)\n",
    "up2 = ConvBlock(128 + 256, 128, time_embed_dim)\n",
    "up1 = ConvBlock(128 + 64 , 64 , time_embed_dim)\n",
    "out = nn.Conv2d(64, in_ch, 1)\n",
    "\n",
    "maxpool = nn.MaxPool2d(2)\n",
    "upsample = nn.Upsample(scale_factor=2, mode='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c42175e-dc9b-42c8-b174-2a6d995be8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 7, 7])\n",
      "torch.Size([1, 256, 3, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 6 but got size 7 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m upsample(x)\n\u001b[0;32m---> 36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m up3(x, v)\n\u001b[1;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m upsample(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 6 but got size 7 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "def _pos_encoding(time_idx, output_dim, device='cpu'):\n",
    "    t, D = time_idx, output_dim\n",
    "    v = torch.zeros(D, device=device)\n",
    "\n",
    "    i = torch.arange(0, D, device=device)\n",
    "    div_term = torch.exp(i / D * math.log(10000))\n",
    "\n",
    "    v[0::2] = torch.sin(t / div_term[0::2])\n",
    "    v[1::2] = torch.cos(t / div_term[1::2])\n",
    "    return v\n",
    "\n",
    "def pos_encoding(timesteps, output_dim, device='cpu'):\n",
    "    batch_size = len(timesteps)\n",
    "    device = timesteps.device\n",
    "    v = torch.zeros(batch_size, output_dim, device=device)\n",
    "    for i in range(batch_size):\n",
    "        v[i] = _pos_encoding(timesteps[i], output_dim, device)\n",
    "    return v\n",
    "\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "v = pos_encoding(timesteps, time_embed_dim, x.device)\n",
    "\n",
    "x1 = down1(x, v)\n",
    "x = maxpool(x1)\n",
    "\n",
    "x2 = down2(x, v)\n",
    "x = maxpool(x2)\n",
    "print(x.shape)\n",
    "x3 = down3(x, v)\n",
    "x = maxpool(x3)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x = upsample(x)\n",
    "x = torch.cat([x, x3], dim=1)\n",
    "x = up3(x, v)\n",
    "\n",
    "x = upsample(x)\n",
    "x = torch.cat([x, x2], dim=1)\n",
    "x = up2(x, v)\n",
    "\n",
    "x = upsample(x)\n",
    "x = torch.cat([x, x1], dim=1)\n",
    "x = up1(x, v)\n",
    "\n",
    "x = out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce4595-2c77-45f1-9b9b-9d4d5c49cf66",
   "metadata": {},
   "source": [
    "単純に、層を増や「そう」としたが、maxpoolingで形状が半分になるため難しい（奇数の場合は切り捨てされる模様）\n",
    "\n",
    "strideを変更すればできなくはない？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229fc408-3a05-440e-a422-850c70498048",
   "metadata": {},
   "source": [
    "upsampleをdecomvolutionに変更させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "79f06dd4-c457-4d70-9130-de13489e111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pos_encoding(time_idx, output_dim, device='cpu'):\n",
    "    t, D = time_idx, output_dim\n",
    "    v = torch.zeros(D, device=device)\n",
    "\n",
    "    i = torch.arange(0, D, device=device)\n",
    "    div_term = torch.exp(i / D * math.log(10000))\n",
    "\n",
    "    v[0::2] = torch.sin(t / div_term[0::2])\n",
    "    v[1::2] = torch.cos(t / div_term[1::2])\n",
    "    return v\n",
    "\n",
    "def pos_encoding(timesteps, output_dim, device='cpu'):\n",
    "    batch_size = len(timesteps)\n",
    "    device = timesteps.device\n",
    "    v = torch.zeros(batch_size, output_dim, device=device)\n",
    "    for i in range(batch_size):\n",
    "        v[i] = _pos_encoding(timesteps[i], output_dim, device)\n",
    "    return v\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_embed_dim):\n",
    "        super().__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(time_embed_dim, in_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_ch, in_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, v):\n",
    "        N, C, _, _ = x.shape\n",
    "        v = self.mlp(v)\n",
    "        v = v.view(N, C, 1, 1)\n",
    "        y = self.convs(x + v)\n",
    "        return y\n",
    "\n",
    "class DeConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_embed_dim):\n",
    "        super().__init__()\n",
    "        self.deconvs = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(time_embed_dim, in_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_ch, in_ch)\n",
    "        )\n",
    "    def forward(self, x, v):\n",
    "        N, C, _, _, = x.shape\n",
    "        # v = self.mlp(v)\n",
    "        # v = v.view(N, C, 1, 1)\n",
    "        y = self.deconvs(x)\n",
    "        return y\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_ch=1, time_embed_dim=100):\n",
    "        super().__init__()\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "\n",
    "        self.down1 = ConvBlock(in_ch, 64, time_embed_dim)\n",
    "        self.down2 = ConvBlock(64, 128, time_embed_dim)\n",
    "        self.bot1 = ConvBlock(128, 256, time_embed_dim)\n",
    "        self.up2 = ConvBlock(128 + 256, 128, time_embed_dim)\n",
    "        self.up1 = ConvBlock(128 + 64, 64, time_embed_dim)\n",
    "        self.out = nn.Conv2d(64, in_ch, 1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        v = pos_encoding(timesteps, self.time_embed_dim, x.device)\n",
    "\n",
    "        x1 = self.down1(x, v)\n",
    "        x = self.maxpool(x1)\n",
    "        x2 = self.down2(x, v)\n",
    "        x = self.maxpool(x2)\n",
    "\n",
    "        x = self.bot1(x, v)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.up2(x, v)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up1(x, v)\n",
    "        x = self.out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f65978ef-895d-460d-9831-16f46b9b8e82",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x100 and 10x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [127]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m deconv \u001b[38;5;241m=\u001b[39m DeConvBlock(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      4\u001b[0m v \u001b[38;5;241m=\u001b[39m pos_encoding(timesteps, time_embed_dim, x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m----> 5\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mdeconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2021.11/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2021.11/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [126]\u001b[0m, in \u001b[0;36mDeConvBlock.forward\u001b[0;34m(self, x, v)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, v):\n\u001b[1;32m     61\u001b[0m     N, C, _, _, \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 62\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(N, C, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     64\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconvs(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2021.11/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2021.11/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2021.11/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2021.11/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2021.11/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2021.11/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x100 and 10x1)"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "deconv = DeConvBlock(1, 64, 10)\n",
    "v = pos_encoding(timesteps, time_embed_dim, x.device)\n",
    "y = deconv(x,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2f4698e6-0575-4c36-8d10-303fc9aef8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 7, 7])\n",
      "torch.Size([1, 64, 7, 7])\n",
      "torch.Size([1, 32, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "v = pos_encoding(timesteps, time_embed_dim, x.device)\n",
    "\n",
    "x1 = down1(x, v)\n",
    "x = maxpool(x1)\n",
    "\n",
    "x2 = down2(x, v)\n",
    "x = maxpool(x2)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "deconv1 = DeConvBlock(128, 64, time_embed_dim)\n",
    "\n",
    "x = deconv1(x, v)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "deconv2 = DeConvBlock(64, 32, time_embed_dim)\n",
    "\n",
    "x = deconv2(x, v)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c619b-2f2a-4cdf-b1f0-b59a398a3c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
