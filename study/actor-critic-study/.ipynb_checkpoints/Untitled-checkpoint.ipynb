{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d02f30-0c1a-4b7d-8d43-5fccfc8263b9",
   "metadata": {},
   "source": [
    "# New env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec6e43e-51ac-4b1b-aa43-7abb95fe6607",
   "metadata": {},
   "source": [
    "# load from prerunning data　bach-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ac0f7b-9902-487f-95be-d0dedf0806da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a4e71d-5a7d-4d0f-9b28-dba362234e0a",
   "metadata": {},
   "source": [
    "# Envのチェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3649dea6-a97e-42c7-9d9b-c6b979a89ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class G4env():\n",
    "    def __init__(self, batch_size = 32):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.angle = np.zeros(self.batch_size)\n",
    "        self.r = 0\n",
    "        self.done = torch.zeros(self.batch_size).view(self.batch_size,1)\n",
    "\n",
    "        self.ang_range = 85\n",
    "        self.event_num = 1000\n",
    "        self.angle_step = 5.0\n",
    "        self.ev_array = -1 * torch.ones((30,31,31))\n",
    "        self.ev_array[15:20, 0:5, 0:5] = 1\n",
    "        \n",
    "        self.dose = dict()\n",
    "        angles = [-90 + 5 * a for a in range(36)]\n",
    "        for angle in angles:\n",
    "            d = pd.read_csv(\"data/dose_t=\" + str(angle) + \".csv\")['dose']\n",
    "            d = np.array(d).reshape(1,30,31,31) / self.event_num\n",
    "            # d = torch.tensor(d)\n",
    "            self.dose[angle] = d\n",
    "\n",
    "    def reset(self):\n",
    "        self.r = torch.zeros((1,30,31,31)).to(device)\n",
    "        self.r_new = 0\n",
    "        self.angle = np.zeros(self.batch_size)\n",
    "        self.done = torch.zeros(self.batch_size).view(self.batch_size,1).to(device)\n",
    "\n",
    "        states = np.array([self.dose[a] for a in self.angle])\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "        \n",
    "        return states\n",
    "    \n",
    "    def GetReward(self, angle):\n",
    "        self.r = self.ev_array * self.dose[angle] #報酬を加算する\n",
    "        reward = torch.tensor(self.r.sum().sum()).view(-1).to(device) #報酬の計算用関数\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        rewards = torch.empty(0).to(device)\n",
    "        self.angle += self.angle_step * (action - 1)\n",
    "        \n",
    "        for a in self.angle:    \n",
    "            \n",
    "            if (np.abs(a) >= self.ang_range):\n",
    "                self.done = 1\n",
    "            \n",
    "            reward = self.GetReward(a)\n",
    "            rewards = torch.cat([rewards, reward],dim=0) #配列を足し合わせる \n",
    "        \n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).view(self.batch_size, 1).to(device)\n",
    "        \n",
    "        states = np.array([self.dose[a] for a in self.angle])\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        return states, rewards, self.done\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d34cb7-0e3b-4b0c-8cac-da9ac7ab68a4",
   "metadata": {},
   "source": [
    "### check env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abe4bf88-0733-4703-ba70-2d62af236638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1919718/2912211575.py:29: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  states = np.array([self.dose[a] for a in self.angle])\n",
      "/tmp/ipykernel_1919718/2912211575.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  states = np.array([self.dose[a] for a in self.angle])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m e \u001b[38;5;241m=\u001b[39m G4env()\n\u001b[0;32m----> 2\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      4\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m32\u001b[39m)\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mG4env.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdose[a] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mangle])\n\u001b[0;32m---> 30\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m states\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "e = G4env()\n",
    "states = e.reset()\n",
    "for _ in range(2):\n",
    "    a = np.random.randint(0,2,32)\n",
    "    next_states = states\n",
    "    states, rewards, dones, = e.step(a)\n",
    "    print(states.shape, rewards.shape, dones.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2991183f-384d-4ce1-9d6f-9ba4eaa1a620",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(G4env().ev_array.sum(0))\n",
    "plt.show()\n",
    "plt.imshow(G4env().ev_array.sum(1))\n",
    "plt.show()\n",
    "plt.imshow(G4env().ev_array.sum(2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ab084-9240-4aeb-a9e8-d0055b446800",
   "metadata": {},
   "source": [
    "# Let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0308b8ff-7ed5-4a38-96e1-4271e183590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWork(nn.Module):\n",
    "    def __init__(self, nch_g=32):\n",
    "        super(NetWork, self).__init__()\n",
    "\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(1        , nch_g    , 2)\n",
    "        self.conv2 = nn.Conv3d(nch_g    , nch_g * 2, 2)\n",
    "        self.conv3 = nn.Conv3d(nch_g * 2, nch_g * 4, 2)\n",
    "\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.flat  = nn.Flatten()\n",
    "\n",
    "        self.policy= nn.Linear(128 * 27 * 28 * 28, 3)\n",
    "        self.value = nn.Linear(128 * 27 * 28 * 28, 1)\n",
    "        self.sfmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #----------------\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        #----------------\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        #----------------\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        #----------------\n",
    "        x = self.flat(x)\n",
    "        #----------------\n",
    "        probs = self.policy(x)\n",
    "        probs = self.sfmax(probs)\n",
    "        \n",
    "        value  = self.value(x)\n",
    "        \n",
    "        return probs, value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f21a29b-6def-4f3b-8473-853816052a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormNetWork(nn.Module):\n",
    "    def __init__(self, nch_g=32):\n",
    "        super(BatchNormNetWork, self).__init__()\n",
    "\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(1        , nch_g    , 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(nch_g)\n",
    "        self.conv2 = nn.Conv3d(nch_g    , nch_g * 2, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(nch_g * 2)\n",
    "        self.conv3 = nn.Conv3d(nch_g * 2, nch_g * 4, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(nch_g * 4)\n",
    "\n",
    "        self.relu  = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "\n",
    "        self.flat  = nn.Flatten()\n",
    "\n",
    "        self.policy= nn.Linear(nch_g * 4 * 9 * 14 * 14, 3)\n",
    "        self.value = nn.Linear(nch_g * 4 * 9 * 14 * 14, 1)\n",
    "        self.sfmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #----------------\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #----------------\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #----------------\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #----------------\n",
    "        x = self.flat(x)\n",
    "        #----------------\n",
    "        probs = self.policy(x)\n",
    "        probs = self.sfmax(probs)\n",
    "        \n",
    "        value  = self.value(x)\n",
    "        \n",
    "        return probs, value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ad823-2cd5-4457-8924-b6273b440064",
   "metadata": {},
   "source": [
    "### Check get action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa2b012d-212c-45c0-a687-e150de75a62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1]) torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "net = NetWork().to(device)\n",
    "probs, _ = net(states)\n",
    "\n",
    "batch_size = states.size(0)\n",
    "actions = torch.multinomial(probs, 1)\n",
    "\n",
    "selected_probs = torch.gather(probs, 1, actions)\n",
    "\n",
    "print(actions.shape, selected_probs.shape)\n",
    "print(rewards.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d12b2-a022-4bb7-a296-806a67d0316c",
   "metadata": {},
   "source": [
    "### Check update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd6b1a1b-9cfb-4783-acfa-b302539a2ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1]) torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.01\n",
    "lr_pi = 0.0002\n",
    "lr_v = 0.0005\n",
    "done = 0\n",
    "\n",
    "optimizer_pi = optim.Adam(net.parameters(), lr=lr_pi)\n",
    "optimizer_v = optim.Adam(net.parameters(), lr=lr_v)\n",
    "\n",
    "# ========== (1) Update V network ============\n",
    "_, v_next_states = net(next_states)\n",
    "print(rewards.shape, v_next_states.shape)\n",
    "print((gamma * v_next_states * (1-done)).shape)\n",
    "with torch.no_grad():\n",
    "    targets = rewards + gamma * v_next_states * (1 - done)\n",
    "_, v_states = net(states)\n",
    "\n",
    "print(targets.shape)\n",
    "loss_v = F.mse_loss(v_states, targets)\n",
    "\n",
    "# ========== (2) Update pi network ===========\n",
    "with torch.no_grad():\n",
    "    deltas = targets - v_states\n",
    "\n",
    "loss_pi = torch.mean(-torch.log(selected_probs) * deltas)\n",
    "\n",
    "# ========== (3) Calculate loss ===============\n",
    "loss = loss_pi + loss_v\n",
    "\n",
    "optimizer_v.zero_grad()\n",
    "optimizer_pi.zero_grad()\n",
    "\n",
    "loss.mean().backward()\n",
    "\n",
    "optimizer_v.step()\n",
    "optimizer_pi.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c73730a-3198-4858-94be-8f91d8c2fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentSingleNet:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.98\n",
    "        self.lr_pi = 0.0000002\n",
    "        self.lr_v = 0.0000005\n",
    "        self.action_size = 3\n",
    "\n",
    "        self.net = NetWork().to(device)\n",
    "        \n",
    "        self.optimizer_pi = optim.Adam(self.net.parameters(), lr=self.lr_pi)\n",
    "        self.optimizer_v = optim.Adam(self.net.parameters(), lr=self.lr_v)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        probs, _ = self.net(state)\n",
    "        \n",
    "        batch_size = state.size(0)\n",
    "        actions = torch.multinomial(probs, 1)\n",
    "        \n",
    "        selected_probs = torch.gather(probs, 1, actions)\n",
    "        return actions, selected_probs\n",
    "        \n",
    "    def update(self, states, actions_probs, rewards, next_states, dones):\n",
    "        # ========== (1) Update V network ============\n",
    "        _, v_next_states = self.net(next_states)\n",
    "        with torch.no_grad():\n",
    "            targets = rewards + self.gamma * v_next_states * (1 - dones)\n",
    "        _, v_states = self.net(states)\n",
    "        loss_v = F.mse_loss(v_states, targets)\n",
    "        \n",
    "        # ========== (2) Update pi network ===========\n",
    "        with torch.no_grad():\n",
    "            deltas = targets - v_states\n",
    "\n",
    "        loss_pi = torch.mean(-torch.log(actions_probs) * deltas)\n",
    "        \n",
    "        # ========== (3) Calculate loss ===============\n",
    "        loss = loss_pi + loss_v\n",
    "        \n",
    "        self.optimizer_v.zero_grad()\n",
    "        self.optimizer_pi.zero_grad()\n",
    "\n",
    "        loss.backward(retain_graph=True)  \n",
    "\n",
    "        self.optimizer_v.step()\n",
    "        self.optimizer_pi.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811f20f4-2d3c-489a-94ff-1b1bd292f47c",
   "metadata": {},
   "source": [
    "### agent test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9040fd7-d9d5-4990-b13b-f857b848a44d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     20\u001b[0m     action, prob \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n\u001b[0;32m---> 21\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     23\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mG4env.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     41\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mangle \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mangle_step \u001b[38;5;241m*\u001b[39m (action \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mangle:    \n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mabs(a) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mang_range):\n",
      "\u001b[0;31mTypeError\u001b[0m: Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations."
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "episodes   = 100\n",
    "batch_size = 32\n",
    "\n",
    "env = G4env(batch_size)\n",
    "agent = AgentSingleNet()\n",
    "\n",
    "reward_lis = []\n",
    "angles  = []\n",
    "begin = time.time()\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    count = 0\n",
    "    print(episode)\n",
    "    while 1:\n",
    "        action, prob = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward.sum()\n",
    "        count += 1\n",
    "\n",
    "        agent.update(state, prob, reward, next_state, done)\n",
    "        state = next_state\n",
    "#         print(\"episode: \",episode,\"step: \", count, \"probs: \", prob, \"actions: \", action, \"angle: \", env.angle)\n",
    "#         print(\"reward: \",reward)\n",
    "        angles.append(env.angle[0])\n",
    "        \n",
    "        if count > 100:\n",
    "            break\n",
    "        if done.sum() > 0:\n",
    "            break\n",
    "    \n",
    "    reward_lis.append(total_reward.item() / batch_size)\n",
    "        \n",
    "    if episode % 10 == 0:\n",
    "        torch.save(agent, \"model02/actor-critic\" + str(episode))\n",
    "end = time.time()\n",
    "print(\"elapsed time is \", end - begin, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23878bd0-2e66-42cb-a8b5-a896276afe37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(angles)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca15bdc-de98-4698-965d-463b5a1d163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4f73f-82d4-4243-aa3b-9a6a8429febf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e50c7-3d5e-4d3e-a29a-32b4e59f66b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1788974b-4f32-4ed8-af7a-dc815f347a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e1817-7a5c-425d-bfed-03e3eb0b57a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_lis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166658e-4954-46da-8631-9f5d26f1f056",
   "metadata": {},
   "source": [
    "# 改善"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b2123-ee63-43bf-9235-0e2e492df850",
   "metadata": {},
   "source": [
    "一定以上の線量が貯まった場合は、スコアを減点する　上限を作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0d9ce-9169-459f-9637-c2c7a6a1255d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
