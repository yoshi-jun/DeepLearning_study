{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57e67d1-35c2-4fff-870b-29313ac04b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e0cd7-4f4d-4433-b752-3e7835808c90",
   "metadata": {},
   "source": [
    "# env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "016cd626-b2c7-4890-81a3-c24f5a7770b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class G4env():\n",
    "    def __init__(self, batch_size = 32):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.angle = np.zeros(self.batch_size)\n",
    "        self.r = torch.empty((1,30,31,31),dtype=torch.float32)\n",
    "        self.done = torch.zeros(self.batch_size).view(self.batch_size,1)\n",
    "\n",
    "        self.ang_range = 85\n",
    "        self.event_num = 1000\n",
    "        self.angle_step = 5.0\n",
    "        self.switch_counter = 0\n",
    "        \n",
    "        # Make evaluate array  \n",
    "        self.ev_array = torch.zeros((30,31,31))\n",
    "        self.ev_array[15:20, 0:5, 0:5] = 1\n",
    "        \n",
    "        # Make dict of dose distribution and angle \n",
    "        self.dose = dict()\n",
    "        angles = [-90 + 5 * a for a in range(36)]\n",
    "        for angle in angles:\n",
    "            d = pd.read_csv(\"data/dose_t=\" + str(angle) + \".csv\")['dose']\n",
    "            d = np.array(d).reshape(1,30,31,31) / self.event_num\n",
    "            # d = torch.tensor(d)\n",
    "            self.dose[angle] = d\n",
    "\n",
    "    def reset(self):\n",
    "        self.r = torch.zeros((1,30,31,31),dtype=torch.float32)\n",
    "        self.switch_counter = 0\n",
    "        \n",
    "        self.angle = np.zeros(self.batch_size)\n",
    "        self.done = torch.zeros(self.batch_size).view(self.batch_size,1).to(device)\n",
    "\n",
    "        states = np.array([self.dose[a] for a in self.angle])\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "        \n",
    "        return states\n",
    "    \n",
    "    def GetReward(self, angle, switch, b):#報酬の計算用関数\n",
    "        \n",
    "        if (switch[b] == 1): #スイッチがONの場合のみ\n",
    "            self.r = self.ev_array * self.dose[angle] #報酬を加算\n",
    "            self.switch_counter += 1\n",
    "        \n",
    "        if (self.switch_counter > 20):\n",
    "            self.done[b] = 1\n",
    "        reward = self.r.to(torch.float32).sum().sum().view(-1)\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def step(self, action, switch):\n",
    "\n",
    "        rewards = torch.empty(0,dtype=torch.float32)\n",
    "        \n",
    "        for b in range(self.batch_size):    \n",
    "            self.angle[b] += self.angle_step * (action[b] - 1)\n",
    "            \n",
    "            if (np.abs(self.angle[b]) >= self.ang_range):\n",
    "                self.done[b] = 1\n",
    "            \n",
    "            reward = self.GetReward(self.angle[b], switch, b)      #Reward を計算\n",
    "            rewards = torch.cat([rewards, reward],dim=0) #配列を組み合わせる(append, push_back) \n",
    "            \n",
    "        rewards = rewards.view(self.batch_size, 1).to(device)\n",
    "        \n",
    "        states = np.array([self.dose[a] for a in self.angle])\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        return states, rewards, self.done\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57265af5-c5b8-4fd4-8311-8745011f51b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchNetWork(nn.Module):\n",
    "    def __init__(self, nch_g=32):\n",
    "        super(SwitchNetWork, self).__init__()\n",
    "\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(1        , nch_g    , 2)\n",
    "        self.conv2 = nn.Conv3d(nch_g    , nch_g * 2, 2)\n",
    "        self.conv3 = nn.Conv3d(nch_g * 2, nch_g * 4, 2)\n",
    "\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.flat  = nn.Flatten()\n",
    "\n",
    "        self.policy= nn.Linear(128 * 27 * 28 * 28, 3)\n",
    "        self.value = nn.Linear(128 * 27 * 28 * 28, 1)\n",
    "        self.switch= nn.Linear(128 * 27 * 28 * 28, 2)\n",
    "        self.sfmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #----------------\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        #----------------\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        #----------------\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        #----------------\n",
    "        x = self.flat(x)\n",
    "        #----------------\n",
    "        probs = self.policy(x)\n",
    "        probs = self.sfmax(probs)\n",
    "        \n",
    "        value  = self.value(x)\n",
    "        \n",
    "        switch = self.switch(x)\n",
    "        switch = self.sfmax(switch)\n",
    "        \n",
    "        return probs, value, switch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2112813-9458-40e1-a07c-464b03980196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentSingleNet:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.98\n",
    "        self.lr_pi = 0.0000002\n",
    "        self.lr_v = 0.0000005\n",
    "        self.action_size = 3\n",
    "\n",
    "        self.net = SwitchNetWork().to(device)\n",
    "        self.reward_max = torch.empty(1).to(device)\n",
    "        self.optimizer_pi = optim.Adam(self.net.parameters(), lr=self.lr_pi)\n",
    "        self.optimizer_v = optim.Adam(self.net.parameters(), lr=self.lr_v)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        probs, _, switch_probs = self.net(state)\n",
    "        \n",
    "        actions = torch.multinomial(probs, 1)\n",
    "        switchs = torch.multinomial(switch_probs, 1)\n",
    "        \n",
    "        selected_probs = torch.gather(probs, 1, actions)\n",
    "        selected_switch_probs = torch.gather(switch_probs, 1, switchs)\n",
    "        \n",
    "        return actions, selected_probs, switchs, selected_switch_probs\n",
    "        \n",
    "    def update(self, states, actions_probs, rewards, next_states, dones):\n",
    "        # ========== (1) Update V network ============\n",
    "        _, v_next_states, _ = self.net(next_states)\n",
    "        with torch.no_grad():\n",
    "            targets = rewards + self.gamma * v_next_states * (1 - dones)\n",
    "        _, v_states, s_probs = self.net(states)\n",
    "        loss_v = F.mse_loss(v_states, targets)\n",
    "        \n",
    "        # ========== (2) Update pi network ===========\n",
    "        with torch.no_grad():\n",
    "            deltas = targets - v_states\n",
    "\n",
    "        loss_pi = torch.mean(-torch.log(actions_probs) * deltas)\n",
    "        \n",
    "        # ==========(2.5) Update Switch network ======\n",
    "        switch_targets = torch.empty(0,dtype=torch.float32)\n",
    "        for reward in rewards:\n",
    "            if (self.reward_max >= reward):\n",
    "                self.reward_max = reward\n",
    "                switch_target = torch.tensor([0,1],dtype=torch.float32).to(device)\n",
    "                switch_targets = torch.cat([switch_targets, switch_target], dim=0)\n",
    "            else:\n",
    "                switch_target = torch.tensor([1,0],dtype=torch.float32).to(device)\n",
    "                switch_targets = torch.cat([switch_targets, switch_target], dim=0)\n",
    "        loss_s = sprobs - switch_targets\n",
    "        \n",
    "        # ========== (3) Calculate loss ===============\n",
    "        loss = loss_pi + loss_v + loss_s\n",
    "        \n",
    "        self.optimizer_v.zero_grad()\n",
    "        self.optimizer_pi.zero_grad()\n",
    "\n",
    "        loss.backward(retain_graph=True)  \n",
    "\n",
    "        self.optimizer_v.step()\n",
    "        self.optimizer_pi.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6adfe9d1-8d67-40ca-b04d-d8deb94a53f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1! (when checking arugment for argument tensors in method wrapper__cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     20\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     24\u001b[0m angles\u001b[38;5;241m.\u001b[39mappend(env\u001b[38;5;241m.\u001b[39mangle[\u001b[38;5;241m0\u001b[39m])\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mAgentSingleNet.update\u001b[0;34m(self, states, actions_probs, rewards, next_states, dones)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         switch_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 47\u001b[0m         switch_targets \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mswitch_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswitch_target\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m loss_s \u001b[38;5;241m=\u001b[39m sprobs \u001b[38;5;241m-\u001b[39m switch_targets\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# ========== (3) Calculate loss ===============\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1! (when checking arugment for argument tensors in method wrapper__cat)"
     ]
    }
   ],
   "source": [
    "episodes   = 5\n",
    "batch_size = 64\n",
    "\n",
    "env = G4env(batch_size)\n",
    "agent = AgentSingleNet()\n",
    "\n",
    "reward_lis = []\n",
    "angles  = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    count = 0\n",
    "    print(episode)\n",
    "    while 1:\n",
    "        action, prob, switch, switch_prob = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action, switch)\n",
    "\n",
    "        total_reward += reward.sum()\n",
    "        count += 1\n",
    "\n",
    "        agent.update(state, prob, reward, next_state, done)\n",
    "        state = next_state\n",
    "        angles.append(env.angle[0])\n",
    "\n",
    "        if count > 100:\n",
    "            break\n",
    "        if done.sum() > 0:\n",
    "            break\n",
    "    \n",
    "    reward_lis.append(total_reward.item() / batch_size)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        # print(\"episode: \",episode,\"step: \", count, \"probs: \", prob, \"actions: \", action, \"angle: \", env.angle)\n",
    "        print(\"reward: \",reward)\n",
    "        # torch.save(agent, \"model03/actor-critic\" + str(episode))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4d6ea-fceb-4786-a5db-c149c243cf15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
