{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609ed56b-09ad-4409-a162-6179d2c4be30",
   "metadata": {},
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584c2c80-0490-41cb-b31d-1ea29de50055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87e76848-349d-446c-8b3e-ae5f884637ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class env():\n",
    "    def __init__(self):\n",
    "        self.angle = 0\n",
    "        self.r = 0\n",
    "        self.done = False\n",
    "\n",
    "        self.ang_range = 45\n",
    "        self.ev_array = np.ones((31,31,30))\n",
    "        \n",
    "    def reset(self):\n",
    "        #ここでもrを計算するために線量分布を計算した方が良いかも？\n",
    "        self.r = 0\n",
    "        self.angle = 0\n",
    "        self.done = False\n",
    "#         cmd = \"source /opt/geant4/re10.07.p03-mt/bin/geant4.sh; ./bin/Application_Main 100 -phi \" + str(self.angle)\n",
    "#         test = subprocess.check_output(cmd, shell = True, executable = \"/bin/bash\")\n",
    "\n",
    "#         dose = pd.readcsv(filename)['dose']\n",
    "#         dose = dose.reshape(31,31,30)\n",
    "        state = torch.rand(1,1,30,30,30)\n",
    "        return state\n",
    "\n",
    "    def step(self,action):\n",
    "        self.angle += 5 * (action - 1)\n",
    "        \n",
    "        if (np.abs(self.angle) > self.ang_range):\n",
    "            self.done = True\n",
    "        \n",
    "        next_state = np.random.rand(30,30,30)\n",
    "\n",
    "        # cmd = \"source /opt/geant4/re10.07.p03-mt/bin/geant4.sh; ./bin/Application_Main 100 -phi \" + str(self.angle)\n",
    "        # test = subprocess.check_output(cmd, shell = True, executable = \"/bin/bash\")\n",
    "        \n",
    "#         dose = pd.readcsv(filename)['dose']\n",
    "#         dose = dose.reshape(31,31,30)\n",
    "        \n",
    "        reword = self.ev_array * next_state\n",
    "        reword = reword.sum() \n",
    "        # self.r = reword - self.r\n",
    "        \n",
    "        return self.dose, reword, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeaf0a77-40a8-43f7-9e66-1499833bc685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    \"\"\"\n",
    "    implements both actor and critic in one model\n",
    "    \"\"\"\n",
    "    def __init__(self, nch_g=64):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleDict({\n",
    "            'Layer1': nn.Sequential(\n",
    "                nn.Conv3d(1, nch_g, 2),\n",
    "                # nn.BatchNorm3d(nch_g),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            'Layer2': nn.Sequential(\n",
    "                nn.Conv3d(nch_g, nch_g * 2, 2),\n",
    "                # nn.BatchNorm3d(nch_g * 2),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            'Layer3': nn.Sequential(\n",
    "                nn.Conv3d(nch_g * 2, nch_g * 4, 2),\n",
    "                # nn.BatchNorm3d(nch_g * 4),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            'Layer4': nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(5038848, 3),#引数の計算が不要になるといい。\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c029fc6f-c035-48f4-8594-e50ec55a1ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3334, 0.3200, 0.3466]], grad_fn=<SoftmaxBackward>)\n",
      "ModuleDict(\n",
      "  (Layer1): Sequential(\n",
      "    (0): Conv3d(1, 64, kernel_size=(2, 2, 2), stride=(1, 1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (Layer2): Sequential(\n",
      "    (0): Conv3d(64, 128, kernel_size=(2, 2, 2), stride=(1, 1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (Layer3): Sequential(\n",
      "    (0): Conv3d(128, 256, kernel_size=(2, 2, 2), stride=(1, 1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (Layer4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=5038848, out_features=3, bias=True)\n",
      "    (2): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pi = PolicyNet()\n",
    "\n",
    "sample_input = torch.randn((1, 1, 30, 30, 30))\n",
    "\n",
    "sample_out = pi(sample_input)\n",
    "\n",
    "print(sample_out)\n",
    "\n",
    "probs = pi(sample_input)\n",
    "\n",
    "# action = torch.multinomial(probs, 1).item()\n",
    "\n",
    "# print(probs, action)\n",
    "\n",
    "print(pi.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187a32a7-46a6-4fde-888f-dc98afea0aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    \"\"\"\n",
    "    implements both actor and critic in one model\n",
    "    \"\"\"\n",
    "    def __init__(self, nch_g=64):\n",
    "        super(ValueNet, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleDict({\n",
    "            'LayerV1': nn.Sequential(\n",
    "                nn.Conv3d(1, nch_g, 2),\n",
    "                # nn.BatchNorm3d(nch_g),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            'LayerV2': nn.Sequential(\n",
    "                nn.Conv3d(nch_g, nch_g * 2, 2),\n",
    "                # nn.BatchNorm3d(nch_g * 2),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            'LayerV3': nn.Sequential(\n",
    "                nn.Conv3d(nch_g * 2, nch_g * 4, 2),\n",
    "                # nn.BatchNorm3d(nch_g * 4),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            'LayerV4': nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(5038848, 1)\n",
    "            )\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7bfb17d-16b8-476b-b70e-2af77e86ae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0169]], grad_fn=<AddmmBackward>)\n",
      "ModuleDict(\n",
      "  (LayerV1): Sequential(\n",
      "    (0): Conv3d(1, 64, kernel_size=(2, 2, 2), stride=(1, 1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (LayerV2): Sequential(\n",
      "    (0): Conv3d(64, 128, kernel_size=(2, 2, 2), stride=(1, 1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (LayerV3): Sequential(\n",
      "    (0): Conv3d(128, 256, kernel_size=(2, 2, 2), stride=(1, 1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (LayerV4): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=5038848, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "v = ValueNet()\n",
    "\n",
    "sample_input = torch.randn((1, 1, 30, 30, 30))\n",
    "\n",
    "sample_out = v(sample_input)\n",
    "\n",
    "print(sample_out)\n",
    "print(v.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9be9960-e732-47a1-9bb7-a081f5d90c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.98\n",
    "        self.lr_pi = 0.0002\n",
    "        self.lr_v = 0.0005\n",
    "        self.action_size = 3\n",
    "\n",
    "        self.pi = PolicyNet()\n",
    "        self.v = ValueNet()\n",
    "        self.optimizer_pi = optim.Adam(self.pi.parameters(), lr=self.lr_pi)\n",
    "        self.optimizer_v = optim.Adam(self.v.parameters(), lr=self.lr_v)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        # state = torch.from_numpy(state).float()\n",
    "        probs = self.pi.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        # action = torch.multinomial(probs, 1).item()\n",
    "        return action, probs\n",
    "\n",
    "    def update(self, state, action_prob, reward, next_state, done):\n",
    "        # state = torch.from_numpy(state).float()\n",
    "        next_state = torch.randn(1, 1, 30, 30, 30)\n",
    "\n",
    "        # ========== (1) Update V network ===========\n",
    "        target = reward + self.gamma * self.v(next_state) * (1 - done)\n",
    "        v = self.v(state)\n",
    "        loss_v = F.mse_loss(v, target.detach())\n",
    "        \n",
    "        # ========== (2) Update pi network ===========\n",
    "        # with torch.no_grad():\n",
    "        delta = target - v\n",
    "        delta.detach()\n",
    "        # loss_pi = -Categorical(probs).log_prob(action) * delta.detach()\n",
    "        \n",
    "        # t = reward + self.gamma * self.pi(next_state) * (1 - done)\n",
    "        print(action_prob)\n",
    "        loss_pi = -torch.log(action_prob[:,action]) * delta.detach()\n",
    "        print(loss_pi)\n",
    "        print(probs)\n",
    "        loss = loss_pi + loss_v\n",
    "        self.optimizer_v.zero_grad()\n",
    "        self.optimizer_pi.zero_grad()\n",
    "        # loss_v.backward()\n",
    "        # loss_pi.backward(retain_graph=True)\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        self.optimizer_v.step()\n",
    "        self.optimizer_pi.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77207882-034d-465b-9cf5-87b5e15ef891",
   "metadata": {},
   "source": [
    "**渡瀬先生が使用しているupdateメソッド**\n",
    "\n",
    "~~~\n",
    "    def update(self, state, action_prob, reward, next_state, done):\n",
    "        #state = state[np.newaxis, :]  # add batch axis ???\n",
    "        #next_state = next_state[np.newaxis, :]???\n",
    "\n",
    "        # ========== (1) Update V network ===========\n",
    "        _,_,_,v1 = self.ac_net(state)\n",
    "        _,_,_,v2 = self.ac_net(next_state)\n",
    "        target = reward + opt.gamma * v2 * (1 - done)\n",
    "        target.detach()\n",
    "        loss_v = mse_loss(v1, target)\n",
    "        \n",
    "        # ========== (2) Update pi network ===========\n",
    "        diff = target - v1\n",
    "        diff.detach()\n",
    "        loss_pi =torch.mean(-torch.log(action_prob) * diff)\n",
    "        loss = loss_v + loss_pi\n",
    "        self.optimizer_ac.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer_ac.step()\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cc6a628-94f0-47e2-a29c-12b41c6a8eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Action: tensor([2])\n",
      "Probability of the Selected Action: tensor([[0.3334, 0.3200, 0.3466]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3334, 0.3200, 0.3466]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[529.7750]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.3334, 0.3200, 0.3466]], grad_fn=<SoftmaxBackward>)\n",
      "Update Successful\n",
      "Selected Action: tensor([2])\n",
      "Probability of the Selected Action: tensor([[0.3334, 0.3200, 0.3466]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3334, 0.3200, 0.3466]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[527.7347]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.3334, 0.3200, 0.3466]], grad_fn=<SoftmaxBackward>)\n",
      "Update Successful\n",
      "state :  tensor(2.1180)\n",
      "Selected Action: tensor([1])\n",
      "Probability of the Selected Action: tensor([[0.3334, 0.3200, 0.3466]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3334, 0.3200, 0.3466]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[566.7744]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.3334, 0.3200, 0.3466]], grad_fn=<SoftmaxBackward>)\n",
      "Update Successful\n",
      "state :  tensor(0.2289)\n",
      "Selected Action: tensor([1])\n",
      "Probability of the Selected Action: tensor([[0.3334, 0.3200, 0.3466]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.3334, 0.3200, 0.3466]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[542.9121]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.3334, 0.3200, 0.3466]], grad_fn=<SoftmaxBackward>)\n",
      "Update Successful\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "state = torch.randn(1, 1, 30, 30, 30)\n",
    "reward = 500\n",
    "next_state = torch.randn(1, 1, 30, 30, 30)\n",
    "done = False\n",
    "\n",
    "agent = Agent()\n",
    "\n",
    "# get_actionメソッドのテスト\n",
    "action, chosen_prob = agent.get_action(state)\n",
    "print(\"Selected Action:\", action)\n",
    "print(\"Probability of the Selected Action:\", probs)\n",
    "\n",
    "# updateメソッドのテスト\n",
    "agent.update(state, probs, reward, next_state, done)\n",
    "print(\"Update Successful\")\n",
    "\n",
    "#==================ここから2回目=======================================\n",
    "torch.manual_seed(0)\n",
    "state = torch.randn(1, 1, 30, 30, 30)\n",
    "next_state = torch.randn(1, 1, 30, 30, 30)\n",
    "\n",
    "# get_actionメソッドのテスト\n",
    "action, chosen_prob = agent.get_action(state)\n",
    "print(\"Selected Action:\", action)\n",
    "print(\"Probability of the Selected Action:\", probs)\n",
    "\n",
    "# updateメソッドのテスト\n",
    "agent.update(state, probs, reward, next_state, done)\n",
    "print(\"Update Successful\")\n",
    "\n",
    "#==================ここから2回目=======================================\n",
    "torch.manual_seed(2010)\n",
    "state = torch.randn(1, 1, 30, 30, 30)\n",
    "print(\"state : \",state[0,0,2,2,2])\n",
    "next_state = torch.randn(1, 1, 30, 30, 30)\n",
    "\n",
    "# get_actionメソッドのテスト\n",
    "action, chosen_prob = agent.get_action(state)\n",
    "print(\"Selected Action:\", action)\n",
    "print(\"Probability of the Selected Action:\", probs)\n",
    "\n",
    "# updateメソッドのテスト\n",
    "agent.update(state, probs, reward, next_state, done)\n",
    "print(\"Update Successful\")\n",
    "\n",
    "#==================ここから2回目=======================================\n",
    "torch.manual_seed(99981)\n",
    "state = torch.randn(1, 1, 30, 30, 30)\n",
    "print(\"state : \",state[0,0,2,2,2])\n",
    "next_state = torch.randn(1, 1, 30, 30, 30)\n",
    "\n",
    "# get_actionメソッドのテスト\n",
    "action, chosen_prob = agent.get_action(state)\n",
    "print(\"Selected Action:\", action)\n",
    "print(\"Probability of the Selected Action:\", probs)\n",
    "\n",
    "# updateメソッドのテスト\n",
    "agent.update(state, probs, reward, next_state, done)\n",
    "print(\"Update Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a2afb83-6e45-4459-8f9a-f2113433e407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu111\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22555a38-0c32-4ea1-ac82-df09e3ad149e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e9214-b9cd-4c72-bcc3-ff037d283fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f124e92-1e10-4ce7-974f-76aa5d61c44b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d97d290-3cff-44fd-8e08-e7a6aa08292c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb8c05-97e9-4f13-bd85-6b9f21e27df3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
